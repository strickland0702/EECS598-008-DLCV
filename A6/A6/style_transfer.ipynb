{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDJwQPZcupab"
   },
   "source": [
    "# EECS 498-008/598-008 Assignment 6-4: Style Transfer\n",
    "\n",
    "Before we start, please put your name and UMID in following format\n",
    "\n",
    ": Firstname LASTNAME, #00000000   //   e.g.) Justin JOHNSON, #12345678"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KMxqLt1h2kx"
   },
   "source": [
    "**Your Answer:**   \n",
    "Hello WORLD, #XXXXXXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ttiie3K-fC6A",
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Style Transfer\n",
    "In this notebook we will implement the style transfer technique from [\"Image Style Transfer Using Convolutional Neural Networks\" (Gatys et al., CVPR 2015)](http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf).\n",
    "\n",
    "The general idea is to take two images, and produce a new image that reflects the content of one but the artistic \"style\" of the other. We will do this by first formulating a loss function that matches the content and style of each respective image in the feature space of a deep network, and then performing gradient descent on the pixels of the image itself.\n",
    "\n",
    "The deep network we use as a feature extractor is [SqueezeNet](https://arxiv.org/abs/1602.07360), a small model that has been trained on ImageNet. You could use any network, but we chose SqueezeNet here for its small size and efficiency.\n",
    "\n",
    "Here's an example of the images you'll be able to produce by the end of this notebook:\n",
    "\n",
    "![caption](http://web.eecs.umich.edu/~justincj/teaching/eecs498/example_styletransfer.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8OM-F8jddQ-"
   },
   "source": [
    "# Setup Code\n",
    "Before getting started we need to run some boilerplate code to set up our environment. You'll need to rerun this setup code each time you start the notebook.\n",
    "\n",
    "First, run this cell load the [autoreload](https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html?highlight=autoreload) extension. This allows us to edit `.py` source files, and re-import them into the notebook for a seamless editing and debugging experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWG4QKDfdfHa"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bzv5-2FDdkAj"
   },
   "source": [
    "### Google Colab Setup\n",
    "\n",
    "Next we need to run a few commands to set up our environment on Google Colab. If you are running this notebook on a local machine you can skip this section.\n",
    "\n",
    "Run the following cell to mount your Google Drive. Follow the link, sign in to your Google account (the same account you used to store this notebook!) and copy the authorization code into the text box that appears below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2BcHlX-dlXe"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RugOWCefdoTP"
   },
   "source": [
    "Now recall the path in your Google Drive where you uploaded this notebook, fill it in below. If everything is working correctly then running the folowing cell should print the filenames from the assignment:\n",
    "\n",
    "```\n",
    "['eecs598', 'network_visualization.py', 'style_transfer.py',  'network_visualization.ipynb', 'a6_helper.py', 'pytorch_autograd_and_nn.py', 'pytorch_autograd_and_nn.ipynb', 'style_transfer.ipynb', 'rnn_lstm_attention_captioning.ipynb',  'rnn_lstm_attention_captioning.py']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h56pMc7zd6uD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# TODO: Fill in the Google Drive path where you uploaded the assignment\n",
    "# Example: If you create a 2020FA folder and put all the files under A1 folder, then '2020FA/A1'\n",
    "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = None\n",
    "GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
    "print(os.listdir(GOOGLE_DRIVE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dwYtYDmId_94"
   },
   "source": [
    "Once you have successfully mounted your Google Drive and located the path to this assignment, run the following cell to allow us to import from the `.py` files of this assignment. If it works correctly, it should print the message:\n",
    "\n",
    "```\n",
    "Hello from style_transfer.py!\n",
    "```\n",
    "\n",
    "as well as the last edit time for the file `style_transfer.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYWkIslOeqYY"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(GOOGLE_DRIVE_PATH)\n",
    "\n",
    "import time, os\n",
    "os.environ[\"TZ\"] = \"US/Eastern\"\n",
    "time.tzset()\n",
    "\n",
    "from style_transfer import *\n",
    "from a6_helper import *\n",
    "hello()\n",
    "\n",
    "py_path = os.path.join(GOOGLE_DRIVE_PATH, 'style_transfer.py')\n",
    "py_edit_time = time.ctime(os.path.getmtime(py_path))\n",
    "print('style_transfer.py last edited on %s' % py_edit_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzqbYcKdz6ew"
   },
   "source": [
    "### Load Packages\n",
    "\n",
    "Run some setup code for this notebook: Import some useful packages and increase the default figure size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q53DlMXboP-T"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import PIL\n",
    "import numpy as np\n",
    "from eecs598.grad import rel_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvUDZWGU3VLV"
   },
   "source": [
    "We will use GPUs to accelerate our computation in this notebook. Run the following to make sure GPUs are enabled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrAX9FOLpr9k"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  print('Good to go!')\n",
    "else:\n",
    "  print('Please set GPU via Edit -> Notebook Settings.')\n",
    "to_float_cuda = torch.cuda.FloatTensor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqm8POJCfC6B",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "### Data Setup\n",
    "\n",
    "Download a few style images and answer checking files. Perform checks for package compatibility with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJUD1pR-ffdO"
   },
   "outputs": [],
   "source": [
    "# download imagenet_val\n",
    "if os.path.isdir('styles'):\n",
    "  print('Style images exist')\n",
    "else:\n",
    "  print('downloading Style images')\n",
    "  # !wget http://web.eecs.umich.edu/~justincj/teaching/eecs498/a4_styles.zip\n",
    "  # !wget http://web.eecs.umich.edu/~justincj/teaching/eecs498/styles.zip\n",
    "  !wget http://web.eecs.umich.edu/~justincj/teaching/eecs498/style_data.zip\n",
    "  !unzip style_data.zip && rm style_data.zip\n",
    "  !mv style_data/* . && rm -rf style_data\n",
    "\n",
    "check_scipy()\n",
    "answers = dict(np.load('style-transfer-checks.npz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvNe7g__cXKQ"
   },
   "source": [
    "### Backbone Setup\n",
    "\n",
    "Load a pre-trained backbone model to extract features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybJ18imQfC6N",
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained SqueezeNet model.\n",
    "cnn = torchvision.models.squeezenet1_1(pretrained=True).features\n",
    "cnn.type(to_float_cuda)\n",
    "\n",
    "# We don't want to train the model any further, so we don't want PyTorch to waste computation \n",
    "# computing gradients on parameters we're never going to update.\n",
    "for param in cnn.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# We provide this helper code which takes an image, a model (cnn), and returns a list of\n",
    "# feature maps, one per layer.\n",
    "def extract_features(x, cnn):\n",
    "    \"\"\"\n",
    "    Use the CNN to extract features from the input image x.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: A PyTorch Tensor of shape (N, C, H, W) holding a minibatch of images that\n",
    "      will be fed to the CNN.\n",
    "    - cnn: A PyTorch model that we will use to extract features.\n",
    "    \n",
    "    Returns:\n",
    "    - features: A list of feature for the input images x extracted using the cnn model.\n",
    "      features[i] is a PyTorch Tensor of shape (N, C_i, H_i, W_i); recall that features\n",
    "      from different layers of the network may have different numbers of channels (C_i) and\n",
    "      spatial dimensions (H_i, W_i).\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    prev_feat = x\n",
    "    for i, module in enumerate(cnn._modules.values()):\n",
    "        next_feat = module(prev_feat)\n",
    "        features.append(next_feat)\n",
    "        prev_feat = next_feat\n",
    "    return features\n",
    "\n",
    "\n",
    "def features_from_img(imgpath, imgsize):\n",
    "    img = preprocess(PIL.Image.open(imgpath), size=imgsize)\n",
    "    img_var = img.type(to_float_cuda)\n",
    "    return extract_features(img_var, cnn), img_var\n",
    "\n",
    "\n",
    "#please disregard warnings about initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uIu-Kq2fC6Q"
   },
   "source": [
    "# Style Transfer\n",
    "## Computing Loss\n",
    "\n",
    "We're going to compute the three components of our loss function now. The loss function is a weighted sum of three terms: content loss + style loss + total variation loss. You'll fill in the functions that compute these weighted terms below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6-FM3KnfC6R",
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "## Content loss\n",
    "We can generate an image that reflects the content of one image and the style of another by incorporating both in our loss function. We want to penalize deviations from the content of the content image and deviations from the style of the style image. We can then use this hybrid loss function to perform gradient descent **not on the parameters** of the model, but instead **on the pixel values** of our original image.\n",
    "\n",
    "Let's first write the content loss function. Content loss measures how much the feature map of the generated image differs from the feature map of the source image. We only care about the content representation of one layer of the network (say, layer $\\ell$), that has feature maps $A^\\ell \\in \\mathbb{R}^{1 \\times C_\\ell \\times H_\\ell \\times W_\\ell}$. $C_\\ell$ is the number of filters/channels in layer $\\ell$, $H_\\ell$ and $W_\\ell$ are the height and width. We will work with reshaped versions of these feature maps that combine all spatial positions into one dimension. Let $F^\\ell \\in \\mathbb{R}^{C_\\ell \\times M_\\ell}$ be the feature map for the current image and $P^\\ell \\in \\mathbb{R}^{C_\\ell \\times M_\\ell}$ be the feature map for the content source image where $M_\\ell=H_\\ell\\times W_\\ell$ is the number of elements in each feature map. Each row of $F^\\ell$ or $P^\\ell$ represents the vectorized activations of a particular filter, convolved over all positions of the image. Finally, let $w_c$ be the weight of the content loss term in the loss function.\n",
    "\n",
    "Then the content loss is given by:\n",
    "\n",
    "$L_c = w_c \\times \\sum_{i,j} (F_{ij}^{\\ell} - P_{ij}^{\\ell})^2$\n",
    "\n",
    "Implement the `content_loss` and run the cell below to test it. You should see errors less than 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fBpKLcKMfC6V"
   },
   "outputs": [],
   "source": [
    "def content_loss_test(correct):\n",
    "    content_image = 'styles/tubingen.jpg'\n",
    "    image_size =  192\n",
    "    content_layer = 3\n",
    "    content_weight = 6e-2\n",
    "    \n",
    "    c_feats, content_img_var = features_from_img(content_image, image_size)\n",
    "    \n",
    "    bad_img = torch.zeros(*content_img_var.data.size()).type(to_float_cuda)\n",
    "    feats = extract_features(bad_img, cnn)\n",
    "    \n",
    "    # YOUR_TURN: Impelement the content_loss function\n",
    "    student_output = content_loss(content_weight, c_feats[content_layer], feats[content_layer])\n",
    "\n",
    "    error = rel_error(correct, student_output)\n",
    "    print('Maximum error is {:.3f}'.format(error))\n",
    "\n",
    "content_loss_test(torch.from_numpy(answers['cl_out']).type(to_float_cuda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ld7SAfmfC6Z"
   },
   "source": [
    "## Style loss\n",
    "Now we can tackle the style loss. For a given layer $\\ell$, the style loss is defined as follows:\n",
    "\n",
    "First, compute the Gram matrix G which represents the correlations between the responses of each filter, where F is as above. The Gram matrix is an approximation to the covariance matrix -- we want the activation statistics of our generated image to match the activation statistics of our style image, and matching the (approximate) covariance is one way to do that. There are a variety of ways you could do this, but the Gram matrix is nice because it's easy to compute and in practice shows good results.\n",
    "\n",
    "Given a feature map $F^\\ell$ of shape $(C_\\ell, M_\\ell)$, the Gram matrix has shape $(C_\\ell, C_\\ell)$ and its elements are given by:\n",
    "\n",
    "$$G_{ij}^\\ell  = \\sum_k F^{\\ell}_{ik} F^{\\ell}_{jk}$$\n",
    "\n",
    "Assuming $G^\\ell$ is the Gram matrix from the feature map of the current image, $A^\\ell$ is the Gram Matrix from the feature map of the source style image, and $w_\\ell$ a scalar weight term, then the style loss for the layer $\\ell$ is simply the weighted Euclidean distance between the two Gram matrices:\n",
    "\n",
    "$$L_s^\\ell = w_\\ell \\sum_{i, j} \\left(G^\\ell_{ij} - A^\\ell_{ij}\\right)^2$$\n",
    "\n",
    "In practice we usually compute the style loss at a set of layers $\\mathcal{L}$ rather than just a single layer $\\ell$; then the total style loss is the sum of style losses at each layer:\n",
    "\n",
    "$$L_s = \\sum_{\\ell \\in \\mathcal{L}} L_s^\\ell$$\n",
    "\n",
    "Begin by implementing `gram_matrix` and run the cell below to test it. You should see errors less than 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0bSzc7ofC6d"
   },
   "outputs": [],
   "source": [
    "def gram_matrix_test(correct):\n",
    "    style_image = 'styles/starry_night.jpg'\n",
    "    style_size = 192\n",
    "    feats, _ = features_from_img(style_image, style_size)\n",
    "    # YOUR_TURN: Impelement the gram_matrix function\n",
    "    student_output = gram_matrix(feats[5].clone())\n",
    "    error = rel_error(correct, student_output)\n",
    "    print('Maximum error is {:.3f}'.format(error))\n",
    "\n",
    "gram_matrix_test(torch.from_numpy(answers['gm_out']).type(to_float_cuda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lYLy1k7fC6l"
   },
   "source": [
    "Next, implement the `style_loss` and run the cell below to test it. The error should be less than 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rl3GvublfC6m"
   },
   "outputs": [],
   "source": [
    "def style_loss_test(correct):\n",
    "    content_image = 'styles/tubingen.jpg'\n",
    "    style_image = 'styles/starry_night.jpg'\n",
    "    image_size =  192\n",
    "    style_size = 192\n",
    "    style_layers = [1, 4, 6, 7]\n",
    "    style_weights = [300000, 1000, 15, 3]\n",
    "    \n",
    "    c_feats, _ = features_from_img(content_image, image_size)    \n",
    "    feats, _ = features_from_img(style_image, style_size)\n",
    "    style_targets = []\n",
    "    for idx in style_layers:\n",
    "        style_targets.append(gram_matrix(feats[idx].clone()))\n",
    "    # YOUR_TURN: Impelement the style_loss function\n",
    "    student_output = style_loss(c_feats, style_layers, style_targets, style_weights)\n",
    "    error = rel_error(correct, student_output)\n",
    "    print('Error is {:.3f}'.format(error))\n",
    "\n",
    "    \n",
    "style_loss_test(torch.from_numpy(answers['sl_out']).type(to_float_cuda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7jBB_XyfC6p"
   },
   "source": [
    "## Total-variation regularization\n",
    "It turns out that it's helpful to also encourage smoothness in the image. We can do this by adding another term to our loss that penalizes wiggles or \"total variation\" in the pixel values. \n",
    "\n",
    "You can compute the \"total variation\" as the sum of the squares of differences in the pixel values for all pairs of pixels that are next to each other (horizontally or vertically). Here we sum the total-variation regularization for each of the 3 input channels (RGB), and weight the total summed loss by the total variation weight, $w_t$:\n",
    "\n",
    "$L_{tv} = w_t \\times \\left(\\sum_{c=1}^3\\sum_{i=1}^{H-1}\\sum_{j=1}^{W} (x_{i+1,j,c} - x_{i,j,c})^2 + \\sum_{c=1}^3\\sum_{i=1}^{H}\\sum_{j=1}^{W - 1} (x_{i,j+1,c} - x_{i,j,c})^2\\right)$\n",
    "\n",
    "Now, implement the `tv_loss` and test it. Error should be less than 0.0001. To receive full credit, your implementation should not have any loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvBkaVFTfC6x"
   },
   "outputs": [],
   "source": [
    "def tv_loss_test(correct):\n",
    "    content_image = 'styles/tubingen.jpg'\n",
    "    image_size =  192\n",
    "    tv_weight = 2e-2\n",
    "\n",
    "    content_img = preprocess(PIL.Image.open(content_image), size=image_size).type(to_float_cuda)\n",
    "    \n",
    "    student_output = tv_loss(content_img, tv_weight)\n",
    "    error = rel_error(correct, student_output)\n",
    "    print('Error is {:.3f}'.format(error))\n",
    "    \n",
    "tv_loss_test(torch.from_numpy(answers['tv_out']).type(to_float_cuda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgwte9i5fC61"
   },
   "source": [
    "Now we're ready to string it all together (you shouldn't have to modify this function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPvMPDVcfC61",
    "scrolled": false,
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "def style_transfer(content_image, style_image, image_size, style_size, content_layer, content_weight,\n",
    "                   style_layers, style_weights, tv_weight, init_random = False, save_image=False, result_filename=None):\n",
    "    \"\"\"\n",
    "    Run style transfer!\n",
    "    \n",
    "    Inputs:\n",
    "    - content_image: filename of content image\n",
    "    - style_image: filename of style image\n",
    "    - image_size: size of smallest image dimension (used for content loss and generated image)\n",
    "    - style_size: size of smallest style image dimension\n",
    "    - content_layer: layer to use for content loss\n",
    "    - content_weight: weighting on content loss\n",
    "    - style_layers: list of layers to use for style loss\n",
    "    - style_weights: list of weights to use for each layer in style_layers\n",
    "    - tv_weight: weight of total variation regularization term\n",
    "    - init_random: initialize the starting image to uniform random noise\n",
    "    - save_image: boolean flag for saving the image\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract features for the content image\n",
    "    content_img = preprocess(PIL.Image.open(content_image), size=image_size).type(to_float_cuda)\n",
    "    feats = extract_features(content_img, cnn)\n",
    "    content_target = feats[content_layer].clone()\n",
    "\n",
    "    # Extract features for the style image\n",
    "    style_img = preprocess(PIL.Image.open(style_image), size=style_size).type(to_float_cuda)\n",
    "    feats = extract_features(style_img, cnn)\n",
    "    style_targets = []\n",
    "    for idx in style_layers:\n",
    "        style_targets.append(gram_matrix(feats[idx].clone()))\n",
    "\n",
    "    # Initialize output image to content image or nois\n",
    "    if init_random:\n",
    "        img = torch.Tensor(content_img.size()).uniform_(0, 1).type(to_float_cuda)\n",
    "    else:\n",
    "        img = content_img.clone().type(to_float_cuda)\n",
    "\n",
    "    # We do want the gradient computed on our image!\n",
    "    img.requires_grad_()\n",
    "    \n",
    "    # Set up optimization hyperparameters\n",
    "    initial_lr = 3.0\n",
    "    decayed_lr = 0.1\n",
    "    decay_lr_at = 180\n",
    "\n",
    "    # Note that we are optimizing the pixel values of the image by passing\n",
    "    # in the img Torch tensor, whose requires_grad flag is set to True\n",
    "    optimizer = torch.optim.Adam([img], lr=initial_lr)\n",
    "    \n",
    "    f, axarr = plt.subplots(1,2)\n",
    "    axarr[0].axis('off')\n",
    "    axarr[1].axis('off')\n",
    "    axarr[0].set_title('Content Source Img.')\n",
    "    axarr[1].set_title('Style Source Img.')\n",
    "    axarr[0].imshow(deprocess(content_img.cpu()))\n",
    "    axarr[1].imshow(deprocess(style_img.cpu()))\n",
    "    plt.show()\n",
    "    plt.figure()\n",
    "    \n",
    "    for t in range(200):\n",
    "        if t < 190:\n",
    "            img.data.clamp_(-1.5, 1.5)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        feats = extract_features(img, cnn)\n",
    "        \n",
    "        # Compute loss\n",
    "        c_loss = content_loss(content_weight, feats[content_layer], content_target)\n",
    "        s_loss = style_loss(feats, style_layers, style_targets, style_weights)\n",
    "        t_loss = tv_loss(img, tv_weight) \n",
    "        loss = c_loss + s_loss + t_loss\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        # Perform gradient descents on our image values\n",
    "        if t == decay_lr_at:\n",
    "            optimizer = torch.optim.Adam([img], lr=decayed_lr)\n",
    "        optimizer.step()\n",
    "\n",
    "        if t % 100 == 0:\n",
    "            print('Iteration {}'.format(t))\n",
    "            plt.axis('off')\n",
    "            plt.imshow(deprocess(img.data.cpu()))\n",
    "            plt.show()\n",
    "    print('Iteration {}'.format(t))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(deprocess(img.data.cpu()))\n",
    "    if save_image:\n",
    "      plt.savefig(os.path.join(GOOGLE_DRIVE_PATH,result_filename))\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6u28v-BVfC64"
   },
   "source": [
    "## Generate some pretty pictures!\n",
    "\n",
    "Try out `style_transfer` on the three different parameter sets below. Make sure to run all three cells. Feel free to add your own, but make sure to include the results of style transfer on the third parameter set (starry night) in your submitted notebook.\n",
    "\n",
    "* The `content_image` is the filename of content image.\n",
    "* The `style_image` is the filename of style image.\n",
    "* The `image_size` is the size of smallest image dimension of the content image (used for content loss and generated image).\n",
    "* The `style_size` is the size of smallest style image dimension.\n",
    "* The `content_layer` specifies which layer to use for content loss.\n",
    "* The `content_weight` gives weighting on content loss in the overall loss function. Increasing the value of this parameter will make the final image look more realistic (closer to the original content).\n",
    "* `style_layers` specifies a list of which layers to use for style loss. \n",
    "* `style_weights` specifies a list of weights to use for each layer in style_layers (each of which will contribute a term to the overall style loss). We generally use higher weights for the earlier style layers because they describe more local/smaller scale features, which are more important to texture than features over larger receptive fields. In general, increasing these weights will make the resulting image look less like the original content and more distorted towards the appearance of the style image.\n",
    "* `tv_weight` specifies the weighting of total variation regularization in the overall loss function. Increasing this value makes the resulting image look smoother and less jagged, at the cost of lower fidelity to style and content. \n",
    "\n",
    "Below the next three cells of code (in which you shouldn't change the hyperparameters), feel free to copy and paste the parameters to play around them and see how the resulting image changes. You will be submitting the style transfer result from the first test for evaluation. Final image will be saved and zipped automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uzvvWwKYfC64"
   },
   "outputs": [],
   "source": [
    "# Composition VII + Tubingen\n",
    "params1 = {\n",
    "    'content_image' : 'styles/tubingen.jpg',\n",
    "    'style_image' : 'styles/composition_vii.jpg',\n",
    "    'image_size' : 192,\n",
    "    'style_size' : 512,\n",
    "    'content_layer' : 3,\n",
    "    'content_weight' : 5e-2, \n",
    "    'style_layers' : (1, 4, 6, 7),\n",
    "    'style_weights' : (20000, 500, 12, 1),\n",
    "    'tv_weight' : 5e-2,\n",
    "    'save_image' : True,\n",
    "    'result_filename' : 'style_transfer_result.jpg'\n",
    "}\n",
    "\n",
    "style_transfer(**params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBUin4IdfC67"
   },
   "outputs": [],
   "source": [
    "# Scream + Tubingen\n",
    "params2 = {\n",
    "    'content_image':'styles/tubingen.jpg',\n",
    "    'style_image':'styles/the_scream.jpg',\n",
    "    'image_size':192,\n",
    "    'style_size':224,\n",
    "    'content_layer':3,\n",
    "    'content_weight':3e-2,\n",
    "    'style_layers':[1, 4, 6, 7],\n",
    "    'style_weights':[200000, 800, 12, 1],\n",
    "    'tv_weight':2e-2,\n",
    "    'save_image' : False\n",
    "}\n",
    "\n",
    "style_transfer(**params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LbAsG4bAfC69"
   },
   "outputs": [],
   "source": [
    "# Starry Night + Tubingen\n",
    "params3 = {\n",
    "    'content_image' : 'styles/tubingen.jpg',\n",
    "    'style_image' : 'styles/starry_night.jpg',\n",
    "    'image_size' : 192,\n",
    "    'style_size' : 192,\n",
    "    'content_layer' : 3,\n",
    "    'content_weight' : 6e-2,\n",
    "    'style_layers' : [1, 4, 6, 7],\n",
    "    'style_weights' : [300000, 1000, 15, 3],\n",
    "    'tv_weight' : 2e-2,\n",
    "    'save_image' : False\n",
    "}\n",
    "\n",
    "style_transfer(**params3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STN3UWY0fC6_",
    "scrolled": true
   },
   "source": [
    "## Feature Inversion\n",
    "\n",
    "The code you've written can do another cool thing. In an attempt to understand the types of features that convolutional networks learn to recognize, a recent paper [1] attempts to reconstruct an image from its feature representation. We can easily implement this idea using image gradients from the pretrained network, which is exactly what we did above (but with two different feature representations).\n",
    "\n",
    "Now, if you set the style weights to all be 0 and initialize the starting image to random noise instead of the content source image, you'll reconstruct an image from the feature representation of the content source image. You're starting with total noise, but you should end up with something that looks quite a bit like your original image.\n",
    "\n",
    "(Similarly, you could do \"texture synthesis\" from scratch if you set the content weight to 0 and initialize the starting image to random noise, but we won't ask you to do that here.) \n",
    "\n",
    "Run the following cell to try out feature inversion and you will submit the result for evaluation. Final image will be saved and zipped automatically.\n",
    "\n",
    "[1] Aravindh Mahendran, Andrea Vedaldi, \"Understanding Deep Image Representations by Inverting them\", CVPR 2015\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BuhhqmjkfC6_"
   },
   "outputs": [],
   "source": [
    "# Feature Inversion -- Starry Night + Tubingen\n",
    "params_inv = {\n",
    "    'content_image' : 'styles/tubingen.jpg',\n",
    "    'style_image' : 'styles/starry_night.jpg',\n",
    "    'image_size' : 192,\n",
    "    'style_size' : 192,\n",
    "    'content_layer' : 3,\n",
    "    'content_weight' : 6e-2,\n",
    "    'style_layers' : [1, 4, 6, 7],\n",
    "    'style_weights' : [0, 0, 0, 0], # we discard any contributions from style to the loss\n",
    "    'tv_weight' : 2e-2,\n",
    "    'init_random': True, # we want to initialize our image to be random\n",
    "    'save_image' : True,\n",
    "    'result_filename' : 'feature_inversion_result.jpg'\n",
    "}\n",
    "\n",
    "style_transfer(**params_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lu8E_u3XlUvt"
   },
   "source": [
    "# Challenge: Spatial Style Transfer\n",
    "**Note: This part only occupies 3% of the whole assignment and won't affect your grade very much. Feel free to skip it.**\n",
    "\n",
    "In this section we will extend the above style transfer technique to introduce control over spatial location, referring to [\"Controlling Perceptual Factors in Neural Style Transfer\" (Gatys et al., CVPR 2017)](https://openaccess.thecvf.com/content_cvpr_2017/papers/Gatys_Controlling_Perceptual_Factors_CVPR_2017_paper.pdf).\n",
    "\n",
    "The general idea of this technique is that instead of getting the artistic style from the whole style image and applying it to the whole content image, we want to acquire the style of a particular region from the style image, and apply it to a corresponding region in the content image. For example, if both our style and content images consist of a sky part and a house part, this technique helps us apply the sky style from the style image to the sky of the content image (see Fig. 2 in the paper)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2BNJHuWkbKh"
   },
   "source": [
    "## Guided Gram Matrix\n",
    "We multiply the feature maps of each layer included in the style features with $R$ guidance channels $T_{\\ell}^r$. A guidance channel is just a image map with [0, 1] values specifying which region gets which style. In other words, the region where the $r^{th}$ content guidance channel equal to one should get the style from the region where the $r^{th}$ style guidance channel equal to 1. With that being said, in this simplified homework problem, we only have $R = 2$, thus we have binary guidance channels. A spatially guided feature map is computed as:\n",
    "\n",
    "$$F_{\\ell}^{r}[:,i] = T_\\ell^r \\circ F_\\ell[:i]$$\n",
    "\n",
    "Here $F_{\\ell}^{r}[:,i]$ means the $i^{th}$ column of $F_\\ell^r$ and $\\circ$ denotes element-wise multiplication. The guided Gram Matrix then becomes:\n",
    "\n",
    "$$G_\\ell^r = (F_\\ell^r)^T F_\\ell^r$$\n",
    "\n",
    "Implement the `guided_gram_matrix` function and test with the cell below, you should see an error less than `1e-4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRFXYybqecsW"
   },
   "outputs": [],
   "source": [
    "sample_feats = torch.tensor(\n",
    "  [[[[[-0.6119, -0.8243, -0.5697],\n",
    "      [-1.3241, -0.0453, -0.3629],\n",
    "      [-0.3321,  0.7076, -0.3311]],\n",
    "\n",
    "      [[ 0.6079, -0.2566,  1.3993],\n",
    "      [ 1.0103, -1.2616, -1.5729],\n",
    "      [-0.0624,  0.4101,  0.4085]],\n",
    "\n",
    "      [[-1.2122,  0.3690, -0.0933],\n",
    "      [ 1.7902, -0.3964, -1.4309],\n",
    "      [ 0.6862,  0.3385,  0.0479]]],\n",
    "\n",
    "\n",
    "      [[[-0.0679, -1.1770,  0.3031],\n",
    "      [ 1.9585, -0.2974, -1.1080],\n",
    "      [-0.8019,  1.3293,  0.3496]],\n",
    "\n",
    "      [[-1.5403, -1.7125,  0.0887],\n",
    "      [-0.7244,  0.8991,  0.6955],\n",
    "      [-1.3828, -0.2718, -1.6329]],\n",
    "\n",
    "      [[-0.5770, -0.0657,  0.5746],\n",
    "      [-0.1939,  0.4295, -0.7639],\n",
    "      [ 0.3080, -0.8542,  0.6180]]]]]\n",
    ")\n",
    "\n",
    "sample_mask = torch.triu(torch.ones((1, 2, 3, 3)))\n",
    "expected_answer = torch.tensor(\n",
    "  [[[[ 0.0601, -0.0172,  0.0375],\n",
    "    [-0.0172,  0.2454,  0.0670],\n",
    "    [ 0.0375,  0.0670,  0.1415]],\n",
    "\n",
    "   [[ 0.1082,  0.0199,  0.0454],\n",
    "    [ 0.0199,  0.3434, -0.0038],\n",
    "    [ 0.0454, -0.0038,  0.0673]]]]\n",
    ")\n",
    "student_answer = guided_gram_matrix(sample_feats, sample_mask)\n",
    "error = rel_error(expected_answer, student_answer)\n",
    "print(f'loss error: {error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHqMm8eVn_Ca"
   },
   "source": [
    "## Guided Style loss\n",
    "After the above guided Gram Matrices are calculated, each of them is now used as the optimization target for the corresponding region of the content image. The style loss for the layer $\\ell$ then becomes:\n",
    "\n",
    "$$L_s^\\ell = w_\\ell \\sum_{r=1}^R\\sum_{i, j} \\left(G^\\ell_{ij} - A^\\ell_{ij}\\right)^2$$\n",
    "\n",
    "Implement the `guided_style_loss` function. It should be almost the same as your `style_loss` function for the original style transfer, except you are now using `guided_gram_matrix` for it. You should see an error less than `1e-6`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuEH9Jd4K7tz"
   },
   "outputs": [],
   "source": [
    "sample_target = torch.tensor(\n",
    "  [[[[ 1.6195,  0.8610, -0.2706],\n",
    "  [ 0.2184, -0.8091, -0.0450],\n",
    "  [-0.0359,  1.7087,  1.7617]],\n",
    "\n",
    "  [[ 1.6495,  1.1240,  1.1872],\n",
    "  [-0.0577, -1.1156, -0.0601],\n",
    "  [-0.7020,  0.4917, -0.3774]]]]\n",
    ")\n",
    "\n",
    "expected_answer = torch.tensor(267.6274)\n",
    "student_answer = guided_style_loss(\n",
    "    [sample_feats], [0], [sample_target], [15], [sample_mask]\n",
    ")\n",
    "error = rel_error(expected_answer, student_answer)\n",
    "print(f'loss error: {error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJrHQcUJqR8W"
   },
   "source": [
    "## Generate Some Images!\n",
    "Now you have implemented everything we need. You can run the below cells to check how the result looks like. In this example, there are two style images, and we are trying to apply the style of house in style1 and the style of sky in style2 to the house and sky in the content image. Essentially it should be similar to Fig.2(f) in the paper.\n",
    "\n",
    "First run some useful functions for the spatial style transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmJ8evUdDTyQ",
    "scrolled": false,
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "def get_image_masks(img_paths, img_size):\n",
    "  masks = []\n",
    "  for path in img_paths:\n",
    "    masks.append(\n",
    "      get_zero_one_masks(PIL.Image.open(path), image_size).type(to_float_cuda)\n",
    "    )\n",
    "  return torch.cat(masks, 0)\n",
    "  \n",
    "def extract_layer_masks(features, masks):\n",
    "  layer_masks = []\n",
    "\n",
    "  for feat in features:\n",
    "    feat_height, feat_width = feat.shape[-2], feat.shape[-1]\n",
    "    feat_transform = T.Resize((feat_height, feat_width))\n",
    "    feat_masks = feat_transform(masks)\n",
    "    layer_masks.append(feat_masks)\n",
    "\n",
    "  return layer_masks\n",
    "\n",
    "\n",
    "def extract_regional_features(x, cnn, R=2):\n",
    "    \"\"\"\n",
    "    Use the CNN to extract features from the input image x.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: A PyTorch Tensor of shape (N, C, H, W) holding a minibatch of images that\n",
    "      will be fed to the CNN.\n",
    "    - cnn: A PyTorch model that we will use to extract features.\n",
    "    - R: stack the features R times to generate the image region guidance\n",
    "    \n",
    "    Returns:\n",
    "    - features: A list of feature for the input images x extracted using the cnn model.\n",
    "      features[i] is a PyTorch Tensor of shape (N, R, C_i, H_i, W_i); recall that features\n",
    "      from different layers of the network may have different numbers of channels (C_i) and\n",
    "      spatial dimensions (H_i, W_i).\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    prev_feat = x\n",
    "    for i, module in enumerate(cnn._modules.values()):\n",
    "        next_feat = module(prev_feat)\n",
    "        features.append(next_feat.unsqueeze(1).repeat(1, R, 1, 1, 1))\n",
    "        prev_feat = next_feat\n",
    "    return features\n",
    "\n",
    "\n",
    "def guided_style_transfer(content_image, style_images, image_size, style_size, \n",
    "                   content_layer, content_weight, content_masks,\n",
    "                   style_layers, style_weights, style_masks, tv_weight, \n",
    "                   init_random = False, save_image=False, result_filename=None):\n",
    "    \"\"\"\n",
    "    Run style transfer!\n",
    "    \n",
    "    Inputs:\n",
    "    - content_image: filename of content image\n",
    "    - style_images: list of filenames of style images\n",
    "    - image_size: size of smallest image dimension (used for content loss and generated image)\n",
    "    - style_size: size of smallest style image dimension\n",
    "    - content_layer: layer to use for content loss\n",
    "    - content_weight: weighting on content loss\n",
    "    - content_masks: binary masks of the content image for the corresponding regions\n",
    "    - style_layers: list of layers to use for style loss\n",
    "    - style_weights: list of weights to use for each layer in style_layers\n",
    "    - style_masks: binary masks of the sylte image for the corresponding regions\n",
    "    - tv_weight: weight of total variation regularization term\n",
    "    - init_random: initialize the starting image to uniform random noise\n",
    "    - save_image: boolean flag for saving the image\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract features for the content image\n",
    "    content_img = preprocess(PIL.Image.open(content_image), size=image_size).type(to_float_cuda)\n",
    "    feats = extract_regional_features(content_img, cnn)\n",
    "    content_target = feats[content_layer].clone()\n",
    "\n",
    "    # Extract features for the style images\n",
    "    # For this simplified assignment problem, we use 2 style images\n",
    "    style1_img, style2_img = [\n",
    "      preprocess(PIL.Image.open(style_image), size=style_size).type(to_float_cuda)\n",
    "      for style_image in style_images\n",
    "    ]\n",
    "    style1_feats = extract_features(style1_img, cnn)\n",
    "    style2_feats = extract_features(style2_img, cnn)\n",
    "\n",
    "    # generate content mask for each shape of the style features\n",
    "    content_masks = extract_layer_masks(style1_feats, content_masks)\n",
    "\n",
    "    # Stack style features\n",
    "    feats = []\n",
    "    for i in range(len(style1_feats)):\n",
    "        style_feats = torch.stack((style1_feats[i], style2_feats[i]), dim=1)\n",
    "        feats.append(style_feats)\n",
    "    style_masks = extract_layer_masks(feats, style_masks)\n",
    "    \n",
    "    style_targets = []\n",
    "    for idx in style_layers:\n",
    "        style_targets.append(guided_gram_matrix(feats[idx].clone(), style_masks[idx]))\n",
    "\n",
    "    # Initialize output image to content image or noise\n",
    "    if init_random:\n",
    "        img = torch.Tensor(content_img.size()).uniform_(0, 1).type(to_float_cuda)\n",
    "    else:\n",
    "        img = content_img.clone().type(to_float_cuda)\n",
    "\n",
    "    # We do want the gradient computed on our image!\n",
    "    img.requires_grad_()\n",
    "    \n",
    "    # Set up optimization hyperparameters\n",
    "    initial_lr = 3.0\n",
    "    decayed_lr = 0.1\n",
    "    decay_lr_at = 180\n",
    "\n",
    "    # Note that we are optimizing the pixel values of the image by passing\n",
    "    # in the img Torch tensor, whose requires_grad flag is set to True\n",
    "    optimizer = torch.optim.Adam([img], lr=initial_lr)\n",
    "    \n",
    "    f, axarr = plt.subplots(1, 3)\n",
    "    axarr[0].axis('off')\n",
    "    axarr[1].axis('off')\n",
    "    axarr[2].axis('off')\n",
    "    axarr[0].set_title('Content Source Img.')\n",
    "    axarr[1].set_title('Style1 Source Img.')\n",
    "    axarr[2].set_title('Style2 Source Img.')\n",
    "    axarr[0].imshow(deprocess(content_img.cpu()))\n",
    "    axarr[1].imshow(deprocess(style1_img.cpu()))\n",
    "    axarr[2].imshow(deprocess(style2_img.cpu()))\n",
    "    plt.show()\n",
    "    plt.figure()\n",
    "    \n",
    "    for t in range(200):\n",
    "        if t < 190:\n",
    "            img.data.clamp_(-1.5, 1.5)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        feats = extract_regional_features(img, cnn)\n",
    "\n",
    "        # Compute loss\n",
    "        c_loss = content_loss(content_weight, feats[content_layer], content_target)\n",
    "        s_loss = guided_style_loss(feats, style_layers, style_targets, style_weights, content_masks)\n",
    "        t_loss = tv_loss(img, tv_weight) \n",
    "        loss = c_loss + s_loss + t_loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform gradient descents on our image values\n",
    "        if t == decay_lr_at:\n",
    "            optimizer = torch.optim.Adam([img], lr=decayed_lr)\n",
    "        optimizer.step()\n",
    "\n",
    "        if t % 100 == 0:\n",
    "            print('Iteration {}'.format(t))\n",
    "            plt.axis('off')\n",
    "            plt.imshow(deprocess(img.data.cpu()))\n",
    "            plt.show()\n",
    "    print('Iteration {}'.format(t))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(deprocess(img.data.cpu()))\n",
    "    if save_image:\n",
    "        plt.savefig(os.path.join(GOOGLE_DRIVE_PATH,result_filename))\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzJ29vgaVCjY"
   },
   "source": [
    "Load some images, including the spatial guidance masks, the content image, and the style images that we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3LxOSXNmZjX"
   },
   "outputs": [],
   "source": [
    "image_folder_path = os.path.join(GOOGLE_DRIVE_PATH, 'images')\n",
    "image_size = (192, 288)\n",
    "content_sky_mask_path = os.path.join(image_folder_path, 'fig2_content_sky.jpg')\n",
    "content_nosky_mask_path = os.path.join(image_folder_path, 'fig2_content_nosky.jpg')\n",
    "content_masks = get_image_masks([content_nosky_mask_path, content_sky_mask_path], image_size).unsqueeze(0)\n",
    "style1_nosky_mask_path = os.path.join(image_folder_path, 'fig2_style1_nosky.jpg')\n",
    "style2_sky_mask_path = os.path.join(image_folder_path, 'fig2_style2_sky.jpg')\n",
    "style_masks = get_image_masks([style1_nosky_mask_path, style2_sky_mask_path], image_size).unsqueeze(0)\n",
    "\n",
    "content_image_path = os.path.join(image_folder_path, 'fig2_content.jpg')\n",
    "style1_image_path = os.path.join(image_folder_path, 'fig2_style1.jpg')\n",
    "style2_image_path = os.path.join(image_folder_path, 'fig2_style2.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nx-9rCD9rOoB"
   },
   "source": [
    "Below are the parameters that we used to generate the spatial style transfer result. You can switch the order of masks when to apply styles to different regions and get a better understanding of how this works. But the image themselves cannot be change, as the spatial guidance masks are pre-generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCpUDFCG3EPW"
   },
   "outputs": [],
   "source": [
    "# Spatial Style Transfer\n",
    "params_inv = {\n",
    "    'content_image' : content_image_path,\n",
    "    'style_images' : [style1_image_path, style2_image_path],\n",
    "    'image_size' : (192, 288),\n",
    "    'style_size' : (192, 288),\n",
    "    'content_layer' : 3,\n",
    "    'content_weight': 2e-2,\n",
    "    'content_masks': content_masks,\n",
    "    'style_layers' : [1, 4, 6, 7],\n",
    "    'style_weights' : [300000, 1000, 15, 3],\n",
    "    # 'style_weights': [0, 0, 0, 0],\n",
    "    'style_masks': style_masks,\n",
    "    'tv_weight' : 2e-2,\n",
    "    'init_random': True, # we want to initialize our image to be random\n",
    "    'save_image' : True,\n",
    "    'result_filename': 'spatial_style_transfer.jpg'\n",
    "}\n",
    "\n",
    "guided_style_transfer(**params_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rn1MxkeeuVLn"
   },
   "source": [
    "# Final checks\n",
    "Make sure all your training results (loss + images) are saved in the notebook. You can run \"Runtime -> Restart and run all...\" to double check before submitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkwitYo42QFr"
   },
   "source": [
    "# Submit Your Work\n",
    "\n",
    "After completing all the notebooks for this assignment, run the following cell to create a .zip file for you to download and turn in. Please MANUALLY SAVE every *.ipynb and *.py files before executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UutkSuTgxAXG"
   },
   "outputs": [],
   "source": [
    "from eecs598.submit import make_a4_submission\n",
    "\n",
    "# TODO: Replace these with your actual uniquename and umid\n",
    "uniquename = None\n",
    "umid = None\n",
    "make_a6_submission(GOOGLE_DRIVE_PATH, uniquename, umid)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "style_transfer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
